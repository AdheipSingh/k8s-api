apiVersion: anchore.com/v1alpha1
kind: AnchoreEngine
metadata:
  name: example-anchoreengine
spec:
  # Default values for anchore_engine chart.

  # Anchore engine has a dependency on Postgresql, configure here
  postgresql:
    # To use an external DB or Google CloudSQL in GKE, uncomment & set 'enabled: false'
    # externalEndpoint, postgresUser, postgresPassword & postgresDatabase are required values for external postgres
    # enabled: false
    postgresUser: anchoreengine
    postgresPassword: anchore-postgres,123
    postgresDatabase: anchore

    # Specify an external (already existing) postgres deployment for use.
    # Set to the host and port. eg. mypostgres.myserver.io:5432
    externalEndpoint: Null

    # Configure size of the persistent volume used with helm managed chart.
    # This should be commented out if using an external endpoint.
    persistence:
      resourcePolicy: nil
      size: 20Gi

    # If running operator on OpenShift - uncomment the image, imageTag & extraEnv values below.
    # image: registry.access.redhat.com/rhscl/postgresql-96-rhel7
    # imageTag: latest
    # extraEnv:
    # - name: POSTGRESQL_USER
    #   value: anchoreengine
    # - name: POSTGRESQL_PASSWORD
    #   value: anchore-postgres,123
    # - name: POSTGRESQL_DATABASE
    #   value: anchore
    # - name: PGUSER
    #   value: postgres
    # - name: LD_LIBRARY_PATH
    #   value: /opt/rh/rh-postgresql96/root/usr/lib64
    # - name: PATH
    #   value: /opt/rh/rh-postgresql96/root/usr/bin:/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

  # Google CloudSQL support in GKE via gce-proxy
  cloudsql:
    # To use CloudSQL in GKE set 'enable: true'
    enabled: false
    # set CloudSQL instance: 'project:zone:instancname'
    instance: ""
    # Optional existing service account secret to use.
    # useExistingServiceAcc: false
    # serviceAccSecretName: service_acc
    # serviceAccJsonName: for_cloudsql.json
    image:
      # set repo and image tag of gce-proxy
      repository: gcr.io/cloudsql-docker/gce-proxy
      tag: 1.12
      pullPolicy: IfNotPresent

  # Create an ingress resource for all external anchore engine services (API & Enterprise UI).
  # By default this chart is setup to use the NGINX ingress controller which needs to be installed & configured on your cluster.
  # To utilize a GCE/ALB ingress controller comment out the nginx annotations below, change ingress.class, edit path configurions as per the comments, & set API/UI services to use NodePort.
  ingress:
    enabled: false
    # Use the following paths for GCE/ALB ingress controller
    # apiPath: /v1/*
    apiPath: /v1/
    # Uncomment the following lines to bind on specific hostnames
    # apiHosts:
    #   - anchore-api.example.com
    annotations:
      # kubernetes.io/ingress.class: gce
      kubernetes.io/ingress.class: nginx
      # nginx.ingress.kubernetes.io/ssl-redirect: "false"
      # kubernetes.io/ingress.allow-http: false
      # kubernetes.io/tls-acme: true
    tls: []
    # Secrets must be manually created in the namespace.
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  # Global configuration shared by all anchore-engine services.
  anchoreGlobal:
    # Image used for all anchore engine deployments (excluding enterprise components).
    image: docker.io/anchore/anchore-engine:v0.4.0
    imagePullPolicy: IfNotPresent

    # Set this value to True to setup the chart for OpenShift deployment compatibility.
    openShiftDeployment: False

    # Set extra environment variables. These will be set on all containers.
    extraEnv: []
      # - name: foo
      #   value: bar

    # Specifies an existing secret to be used for admin and db passwords
    existingSecret: ""

    # The scratchVolume controls the mounting of an external volume for scratch space for image analysis. Generally speaking
    # you need to provision 3x the size of the largest image (uncompressed) that you want to analyze for this space.
    scratchVolume:
      mountPath: /analysis_scratch
      details:
        # Specify volume configuration here
        emptyDir: {}

    ###
    # Start of General Anchore Engine Configurations (populates /config/config.yaml)
    ###
    # Set where default configs are placed at startup. This must be a writable location for the pod.
    serviceDir: /anchore_service
    logLevel: INFO

    # If true, when a user adds an ECR registry with username = awsauto then the system will look for an instance profile to use for auth against the registry
    allowECRUseIAMRole: false

    # Enable prometheus metrics
    enableMetrics: false

    # Sets the password & email address for the default anchore-engine admin user.
    defaultAdminPassword: foobar
    defaultAdminEmail: example@email.com

    # Configure the database connection within anchore-engine & enterprise-ui. This may get split into 2 different configurations based on service utilized.
    dbConfig:
      timeout: 120
      # Use ssl, but the default postgresql config in helm's stable repo does not support ssl on server side, so this should be set for external dbs only for the time being
      ssl: false
      connectionPoolSize: 30
      connectionPoolMaxOverflow: 100

    # Enable to force all anchore-engine services to communicate internally using SSL
    internalServicesSslEnabled: false
    internalServicesSsl:
      # specify whether cert is verified against the local certificate bundle (allow self-signed certs if set to false)
      verifyCerts: false
      certDir: /home/anchore/certs
      certSecret: Null
      certSecretKeyName: tls.key
      certSecretCertName: tls.crt

    # To enable webhooks, set webhooksEnabled: true
    webhooksEnabled: false
    # Configure webhook outputs here. The service provides these webhooks for notifying external systems of updates
    webhooks:
      # User and password to be set (using HTTP basic auth) on all webhook calls if necessary
      webhook_user: Null
      webhook_password: Null
      ssl_verify: true

      # Endpoint for general notification delivery. These events are image/tag updates etc. This is globally configured
      # and updates for all users are sent to the same host but with a different path for each user.
      # <notification_type>/<userId> are required as documented at end of URI - only hostname:port should be configured.
      general: {}
        # url: "http://somehost:9090/<notification_type>/<userId>"

  # Configuration for the analyzer pods that perform image analysis
  # There may be many of these analyzers but best practice is to not have more than one per node since analysis
  # is very IO intensive. Use of affinity/anti-affinity rules for scheduling the analyzers is future work.
  anchoreAnalyzer:
    replicaCount: 1
    containerPort: 8084

    # Set extra environment variables. These will be set only on analyzer containers.
    extraEnv: []
      # - name: foo
      #   value: bar

    # The cycle timer is the interval between checks to the work queue for new jobs
    cycleTimers:
      image_analyzer: 5

    # Controls the concurrency of the analyzer itself. Can be configured to process more than one task at a time, but it IO bound, so may not
    # necessarily be faster depending on hardware. Should test and balance this value vs. number of analyzers for your deployment cluster performance.
    concurrentTasksPerWorker: 1

    # Image layer caching can be enabled to speed up image downloads before analysis.
    # This chart sets up a scratch directory for all analyzer pods using the values found at anchoreGlobal.scratchVolume.
    # When setting anchoreAnalyzer.layerCacheMaxGigabytes, ensure the scratch volume has suffient storage space.
    # For more info see - https://docs.anchore.com/current/docs/engine/engine_installation/storage/layer_caching/
    # Enable image layer caching by setting a cache size > 0GB.
    layerCacheMaxGigabytes: 0

    # resources:
    #  limits:
    #    cpu: 100m
    #    memory: 8Gi
    #  requests:
    #    cpu: 100m
    #    memory: 3Gi

    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}


  # Pod configuration for the anchore engine api service.
  anchoreApi:
    replicaCount: 1

    # Set extra environment variables. These will be set on all api containers.
    extraEnv: []
      # - name: foo
      #   value: bar

    # kubernetes service configuration for anchore external API
    service:
      type: ClusterIP
      port: 8228
      annotations: {}

    # resources:
    #  limits:
    #    cpu: 100m
    #    memory: 6Gi
    #  requests:
    #    cpu: 100m
    #    memory: 4Gi

    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  anchoreCatalog:
    replicaCount: 1

    # Set extra environment variables. These will be set on all catalog containers.
    extraEnv: []
      # - name: foo
      #   value: bar

    # Intervals to run specific events on (seconds)
    cycleTimers:
      # Interval to check for an update to a tag
      image_watcher: 3600
      # Interval to run a policy evaluation on images with the policy_eval subscription activated.
      policy_eval: 3600
      # Interval to run a vulnerability scan on images with the vuln_update subscription activated.
      vulnerability_scan: 14400
      # Interval at which the catalog looks for new work to put on the image analysis queue.
      analyzer_queue: 1
      # Interval notifications will be processed for state changes
      notifications: 30
      # Intervals service state updates are polled for the system status
      service_watcher: 15
      # Interval between checks to repo for new tags
      repo_watcher: 60

    # Event log configuration for webhooks
    events:
      notification:
        enabled: false
        # Send notifications for events with severity level that matches items in this list
        level:
          - error
          # - info

    archive:
      compression:
        enabled: true
        min_size_kbytes: 100
      storage_driver:
        # Valid storage driver names: 'db', 's3', 'swift'
        name: db
        config: {}

        # Example S3 Configuration:
        # name: s3
        # config:
        #   # All objects are stored in a single bucket, defined here
        #   bucket: "anchore-engine-testing"
        #   # A prefix for keys in the bucket if desired (optional)
        #   prefix: "internaltest"
        #   # Create the bucket if it doesn't already exist
        #   create_bucket: false
            # Url only needed for non-AWS S3 implementations (e.g. minio). Otherwise, configure the region instead
        #   #url: "https://s3.amazonaws.com"
        #   # AWS region to connect to if 'url' not specified, if both are set, then 'url' has precedent
        #   region: us-west-2
        #   # For Auth can provide access/secret keys or use 'iamauto' which will use an instance profile or any credentials found in normal aws search paths/metadata service
        #   access_key: XXXX
        #   secret_key: YYYY
        #   iamauto: false

        # Example Minio configuration (basically same as s3 example):
        # name: s3
        # config:
        #   url: http://<minio url>:9000
        #   bucket: mybucket
        #   access_key: xxxxxx
        #   secret_key: yyyyyy
        #   create_bucket: true

        # Example Swift Configuration:
        # name: swift
        # config:
        #     # Config for swift has a few options, just add the keys and names as used to configure a swiftclient here. All are passed directly to the client impl.
        #     user: "test:tester"
        #     key: "testing"
        #     auth: "http://swift_ephemeral:8080/auth/v1.0"
        #     # The swift container where data will be stored
        #     container: "local_test_anchore"
        #     # Create the container if it is not already present
        #     create_container: false

    # kubernetes service configuration for anchore catalog api
    service:
      type: ClusterIP
      port: 8082
      annotations: {}

    # resources:
    #  limits:
    #    cpu: 100m
    #    memory: 8Gi
    #  requests:
    #    cpu: 100m
    #    memory: 3Gi

    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  # Pod configuration for the anchore engine policy service.
  anchorePolicyEngine:
    replicaCount: 1

    # Set extra environment variables. These will be set on all policy engine containers.
    extraEnv: []
      # - name: foo
      #   value: bar

    # Intervals to run specific events on (seconds)
    cycleTimers:
      # Interval to run a feed sync to get latest cve data
      feed_sync: 14400
      # Interval between checks to see if there needs to be a task queued
      feed_sync_checker: 3600

    # kubernetes service configuration for anchore policy engine api
    service:
      type: ClusterIP
      port: 8087
      annotations: {}

    # resources:
    #  limits:
    #    cpu: 100m
    #    memory: 8Gi
    #  requests:
    #    cpu: 100m
    #    memory: 3Gi

    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

  # Pod configuration for the anchore engine simplequeue service.
  anchoreSimpleQueue:
    replicaCount: 1

    # Set extra environment variables. These will be set on all simplequeue containers.
    extraEnv: []
      # - name: foo
      #   value: bar

    # kubernetes service configuration for anchore simplequeue api
    service:
      type: ClusterIP
      port: 8083
      annotations: {}

    # resources:
    #  limits:
    #    cpu: 100m
    #    memory: 3Gi
    #  requests:
    #    cpu: 100m
    #    memory: 2Gi

    annotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
